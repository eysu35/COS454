{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## Libraries ##\n",
    "###############\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105107a",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8488648",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "## Load, partition, and resize MNIST Digits ##\n",
    "##############################################\n",
    "def loadData():\n",
    "    all_data = np.load(\"/scratch/gpfs/eysu/src_data/mnist.npz\")\n",
    "\n",
    "    x_test = all_data['x_test']\n",
    "    x_train = all_data['x_train']\n",
    "    y_train = all_data['y_train']\n",
    "    y_test = all_data['y_test']\n",
    "\n",
    "    labels = [\"0\",  # index 0\n",
    "              \"1\",  # index 1\n",
    "              \"2\",  # index 2 \n",
    "              \"3\",  # index 3 \n",
    "              \"4\",  # index 4\n",
    "              \"5\",  # index 5\n",
    "              \"6\",  # index 6 \n",
    "              \"7\",  # index 7 \n",
    "              \"8\",  # index 8 \n",
    "              \"9\"]  # index 9\n",
    "\n",
    "    # save train labels\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "\n",
    "\n",
    "    # y_train_labels = y_train\n",
    "    # y_test_labels = y_test\n",
    "\n",
    "    # Further break training data into train / validation sets (# put 5000 into validation set and keep remaining 55,000 for train)\n",
    "    (x_train, x_valid) = x_train[5000:], x_train[:5000] \n",
    "    (y_train, y_valid) = y_train[5000:], np.array(y_train[:5000]).squeeze()\n",
    "\n",
    "    # Reshape input data from (28, 28) to (28, 28, 1)\n",
    "    w, h = 28, 28\n",
    "    x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
    "    \n",
    "    return x_train, x_valid, x_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21b4e2",
   "metadata": {},
   "source": [
    "# Pretrain the model on a few of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e06e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "## PRETRAINING STEP 1: isolate a subset of the data ##\n",
    "######################################################\n",
    "\n",
    "## Pretrain the model on incrementing slices of data\n",
    "for i in range(1,11):\n",
    "    # set END_IDX to the size of the pretraining subset\n",
    "    END_IDX = i\n",
    "    \n",
    "    # load in MNIST digit data\n",
    "    x_train, x_valid, x_test, y_train, y_valid, y_test = loadData()\n",
    "  \n",
    "    ###########################################\n",
    "    ## Slice the train, valid, and test data ##\n",
    "    ## based on the END_IDX                  ##\n",
    "    ###########################################\n",
    "    \n",
    "    idx_train = np.where(y_train < END_IDX)\n",
    "    y_train_subset = y_train[idx_train]\n",
    "    x_train_subset = x_train[idx_train]\n",
    "\n",
    "    idx_valid = np.where(y_valid < END_IDX)\n",
    "    y_valid_subset = y_valid[idx_valid]\n",
    "    x_valid_subset = x_valid[idx_valid]\n",
    "\n",
    "    idx_test = np.where(y_test < END_IDX)\n",
    "    y_test_subset = y_test[idx_test]\n",
    "    x_test_subset = x_test[idx_test]\n",
    "    \n",
    "    # pretrain the model on the data subset and save weights\n",
    "    pretrain(x_train_subset, y_train_subset, x_valid_subset, y_valid_subset, x_test_subset, y_test_subset, END_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e54ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "## PRETRAINING STEP 2: train data on data subset ##\n",
    "###################################################\n",
    "def pretrain(x_train_subset, y_train_subset, x_valid_subset, y_valid_subset, x_test_subset, y_test_subset, end_idx):\n",
    "    \n",
    "    # Define the model: a small CNN model\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Must define the input shape in the first layer of the neural network\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Take a look at the model summary\n",
    "    # model.summary()\n",
    "\n",
    "    # Number of epochs per training run\n",
    "    EPOCHS = 10\n",
    "    # Where to store output model weights and softmax predictions\n",
    "    # Save each set of weights under the relevant folder\n",
    "    save_path = \"/scratch/gpfs/eysu/low_shot_weights/\" + str(end_idx) + \"/\"\n",
    "\n",
    "    # fix dimensions\n",
    "    y_train_subset = tf.keras.utils.to_categorical(y_train_subset, 10)\n",
    "    y_valid_subset = tf.keras.utils.to_categorical(y_valid_subset, 10)\n",
    "    y_test_subset = tf.keras.utils.to_categorical(y_test_subset, 10)\n",
    "\n",
    "    mpth = 'model.weights.best.pretrain.hdf5'\n",
    "    y_hat_test_name = 'y_hat_test_pretrain'\n",
    "    y_hat_train_name = 'y_hat_train_pretrain'\n",
    "\n",
    "\n",
    "    # define optimization and energy parameters\n",
    "    # set learning rate and exponential decay rate \n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001, beta_1 =0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    # Save checkpoints\n",
    "    checkpointer = ModelCheckpoint(filepath= save_path + mpth, verbose = 1, save_best_only=True) #True\n",
    "    # Train the model\n",
    "    model.fit(x_train_subset,\n",
    "             y_train_subset,\n",
    "             batch_size=64,\n",
    "             epochs=EPOCHS,\n",
    "             validation_data=(x_valid_subset, y_valid_subset),\n",
    "             callbacks=[checkpointer])\n",
    "\n",
    "    # y_hat = model.predict(x_train_subset) #feed back serial reproduction targets\n",
    "    y_hat_test = model.predict(x_test_subset)\n",
    "\n",
    "    # Load the weights with the best validation accuracy\n",
    "    model.load_weights(save_path + mpth)\n",
    "    # Evaluate the model on test set\n",
    "    score = model.evaluate(x_test_subset, y_test_subset, verbose=0)\n",
    "    # Print test accuracy\n",
    "    print('\\n', 'Test accuracy:', score[1]) \n",
    "\n",
    "    # Save results for each iteration in the serial reproduction chain\n",
    "    np.save(save_path + y_hat_train_name + '.npy', y_train_subset)\n",
    "    print(save_path + y_hat_train_name)\n",
    "\n",
    "    np.save(save_path + y_hat_test_name + '.npy', y_hat_test)\n",
    "    print(save_path + y_hat_test_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea6bcb",
   "metadata": {},
   "source": [
    "# Iterated Retraining Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447429ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "## Create a Shuffled y_train such that the training images ##\n",
    "## and labels no longer match up                           ##\n",
    "#############################################################\n",
    "def shuffle_all(y_train):\n",
    "    y_train_shuffle = np.copy(y_train)\n",
    "    np.random.shuffle(y_train_shuffle)\n",
    "    \n",
    "    return y_train_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "## Create a Shuffled y_train such that the training images ##\n",
    "## and labels no longer match up ONLY for the unseen images##\n",
    "#############################################################\n",
    "def shuffle_some(x_train, y_train, END_IDX):\n",
    "    # indices of the images that the model was pretrained on\n",
    "    idx_train = np.where(y_train < END_IDX)[0]\n",
    "\n",
    "    # split training data and randomize labels of unseen \n",
    "    x_train_true = x_train[idx_train]\n",
    "    y_train_true = y_train[idx_train]\n",
    "    x_train_shuffle = np.delete(x_train, idx_train, axis=0)\n",
    "    y_train_shuffle = np.delete(y_train, idx_train, axis=0)\n",
    "    np.random.shuffle(y_train_shuffle)\n",
    "\n",
    "    # recombine the data such that some of the labels are true and others are randomized\n",
    "    x_train = np.concatenate((x_train_true, x_train_shuffle), axis=0)\n",
    "    y_train = np.concatenate((y_train_true, y_train_shuffle))\n",
    "\n",
    "    # mix the true and randomized labels \n",
    "    mix_order = np.random.permutation(len(y_train))\n",
    "    x_train = x_train[mix_order]\n",
    "    y_train = y_train[mix_order]\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38155d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "## This cell runs the iterated learning training procedure. ##\n",
    "## USING RANDOMIZED LABELS.                                 ##\n",
    "##############################################################\n",
    "for END_IDX in range(1, 11):\n",
    "    # Number of learning iterations\n",
    "    MAX_ITER = 25\n",
    "    # Number of epochs per training run (decrease this to learn less)\n",
    "    EPOCHS = 10\n",
    "    # Where to store output model weights and softmax predictions\n",
    "    save_path = \"/scratch/gpfs/eysu/low_shot_weights/\" + str(END_IDX) + \"/\"\n",
    "    \n",
    "    x_train, x_valid, x_test, y_train, y_valid, y_test = loadData()\n",
    "    \n",
    "    # y_train = shuffle_all(y_train)\n",
    "    \n",
    "#     x_train, y_train = shuffle_some(x_train, y_train, END_IDX)\n",
    "\n",
    "    for iteration in range(0,MAX_ITER):\n",
    "        # If iteration is seed, train on original target vectors, else, train on y_hat from time t-1\n",
    "        if iteration == 0:\n",
    "            # One-hot encode the labels\n",
    "            # Pass the randomized labels to the model as y_train\n",
    "            y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "            y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "            y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "            mpth = 'model.weights.best.hdf5'\n",
    "            y_hat_test_name = 'y_hat_test_seed'\n",
    "            y_hat_train_name = 'y_hat_train_seed'      \n",
    "        elif iteration > 0:\n",
    "            # Key step: set new targets as y_hat\n",
    "            y_train = y_hat    \n",
    "            mpth = 'model.weights.best.' + 'iter' + str(iteration) + '.hdf5'\n",
    "            y_hat_test_name = 'y_hat_test_' + 'iter' + str(iteration)\n",
    "            y_hat_train_name = 'y_hat_train_' + 'iter' + str(iteration)\n",
    "\n",
    "        # Define the model: a small CNN model\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        # Must define the input shape in the first layer of the neural network\n",
    "        model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "        model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "        model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "        model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "        # Each time, use the pretrained model with the prior from the lo shot training\n",
    "        model.load_weights(save_path + 'model.weights.best.pretrain.hdf5')\n",
    "        # model.summary()\n",
    "\n",
    "        # define optimization and energy parameters\n",
    "        # default learning_Rate = 0.001\n",
    "        # default beta_1 = 0.9\n",
    "        # reduce both significantly to slow down learning! *** BORROWING FROM INTERLEAVING PAPER **** , \n",
    "\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.0001, beta_1 = 0.0001)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "        # Save checkpoints\n",
    "        checkpointer = ModelCheckpoint(filepath= save_path + \"shuffle_none/\" + mpth, verbose = 1, save_best_only=False) #True\n",
    "        # Train the model\n",
    "        model.fit(x_train,\n",
    "                 y_train,\n",
    "                 batch_size=64,\n",
    "                 epochs=EPOCHS,\n",
    "                 validation_data=(x_valid, y_valid),\n",
    "                 callbacks=[checkpointer])\n",
    "\n",
    "        # Load the weights with the best validation accuracy\n",
    "        y_hat = model.predict(x_train) #feed back serial reproduction targets\n",
    "        y_hat_test = model.predict(x_test)\n",
    "\n",
    "        model.load_weights(save_path + mpth)\n",
    "        # Evaluate the model on test set\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        # Print test accuracy\n",
    "        print('\\n', 'Test accuracy:', score[1])\n",
    "\n",
    "        # Save results for each iteration in the serial reproduction chain\n",
    "        np.save(save_path + \"shuffle_none/\" + y_hat_train_name + '.npy', y_train)\n",
    "        print(save_path + \"shuffle_none/\" + y_hat_train_name)\n",
    "\n",
    "        np.save(save_path + \"shuffle_none/\" + y_hat_test_name + '.npy', y_hat_test)\n",
    "        print(save_path + \"shuffle_none/\" + y_hat_test_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d9f53",
   "metadata": {},
   "source": [
    "# Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a90a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "## Visualize N images ##\n",
    "########################\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.backends.backend_pdf\n",
    "\n",
    "def visualize_softmax(image_idx):\n",
    "\n",
    "    # visualize each image\n",
    "    figure = plt.figure(figsize=(40, 40))\n",
    "    # plot image\n",
    "    ax1 = figure.add_subplot(8, 8, 1, xticks=[], yticks=[])\n",
    "    im1 = ax1.imshow(x_train[image_idx])\n",
    "    ax1.set_title(\"Image\")\n",
    "\n",
    "    # plot weights graph\n",
    "    ax2 = figure.add_subplot(8, 8, 2)\n",
    "    im2 = ax2.imshow(y_hat_train_arr[image_idx, :, :].T, cmap='Wistia')\n",
    "\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    cbar = figure.colorbar(im2, cax=cax, orientation='vertical', ticks=[0, 1])\n",
    "    cbar.ax.set_yticklabels(['0', '1'])\n",
    "\n",
    "    ax2.set(xlabel='Classes', ylabel='Iterations', title='Softmax Outputs')\n",
    "    ax2.set_xticks(ticks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    ax2.set_xticklabels(labels)\n",
    "\n",
    "    pdf.savefig(figure, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "## Look at softmax output matrices for all sets of images ##\n",
    "############################################################\n",
    "\n",
    "# define constants and params\n",
    "MAX_ITER = 25\n",
    "labels = [\"0\",  # index 0\n",
    "              \"1\",  # index 1\n",
    "              \"2\",  # index 2 \n",
    "              \"3\",  # index 3 \n",
    "              \"4\",  # index 4\n",
    "              \"5\",  # index 5\n",
    "              \"6\",  # index 6 \n",
    "              \"7\",  # index 7 \n",
    "              \"8\",  # index 8 \n",
    "              \"9\"]  # index 9\n",
    "\n",
    "# for every pretrained model, generate images for random, \n",
    "# semi random, and non random training processes\n",
    "for END_IDX in range(10, 11):\n",
    "\n",
    "    x_train, x_valid, x_test, y_train, y_valid, y_test = loadData()\n",
    "    # create empty array to store softmax outputs\n",
    "    y_hat_train_arr = np.zeros([y_train.shape[0], 10, MAX_ITER])\n",
    "    \n",
    "    for case in [\"LR_adjusted/\",\"shuffle_some/\", \"shuffle_none/\"]:\n",
    "        save_path = \"/scratch/gpfs/eysu/low_shot_weights/\" + str(END_IDX) + \"/\" + case\n",
    "        \n",
    "        edit_name = False\n",
    "        if case == \"LR_adjusted/\":\n",
    "            edit_name = True\n",
    "            \n",
    "        # store the softmax vector from every iteration of training into y_hat_train_arr\n",
    "        for i in range(MAX_ITER):\n",
    "            if i == 0:\n",
    "                if edit_name: y_hat_train_name = 'LR_adjustedy_hat_train_seed'\n",
    "                else: y_hat_train_name = 'y_hat_train_seed'\n",
    "\n",
    "            else:\n",
    "                if edit_name: y_hat_train_name = 'LR_adjustedy_hat_train_' + 'iter' + str(i)\n",
    "                else: y_hat_train_name = 'y_hat_train_' + 'iter' + str(i)\n",
    "\n",
    "            # Load test set softmax outputs \n",
    "            yhtr = np.load(save_path + y_hat_train_name + '.npy')\n",
    "            \n",
    "            if i == 0:\n",
    "                true_class_tr = np.nonzero(yhtr)[1]   \n",
    "            y_hat_train_arr[:, :, i] = yhtr\n",
    "            \n",
    "        print(str(END_IDX) + \":\" + case)\n",
    "        save_image_path = \"Outputs/\" + case + str(END_IDX) + \".pdf\"\n",
    "        pdf = matplotlib.backends.backend_pdf.PdfPages(save_image_path)\n",
    "        for j in range(100):\n",
    "            visualize_softmax(np.random.randint(0, y_hat_train_arr.shape[0]))\n",
    "        pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb8500c",
   "metadata": {},
   "source": [
    "# More Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## Compare Softmax Matrices ##\n",
    "##############################\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.backends.backend_pdf\n",
    "\n",
    "def visualize_all_softmax(image_idx):\n",
    "\n",
    "    # visualize each image\n",
    "    figure = plt.figure(figsize=(40, 40))\n",
    "    # plot image\n",
    "    ax1 = figure.add_subplot(8, 8, 1, xticks=[], yticks=[])\n",
    "    im1 = ax1.imshow(x_train[image_idx])\n",
    "    ax1.set_title(\"Image\")\n",
    "\n",
    "    # plot weights graph\n",
    "    ax2 = figure.add_subplot(8, 8, 2)\n",
    "    im2 = ax2.imshow(y_hat_train_arr_LR[image_idx, :, :].T, cmap='Wistia')\n",
    "\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    cbar = figure.colorbar(im2, cax=cax, orientation='vertical', ticks=[0, 1])\n",
    "    cbar.ax.set_yticklabels(['0', '1'])\n",
    "\n",
    "    ax2.set(xlabel='Classes', ylabel='Iterations', title='Softmax Outputs')\n",
    "    ax2.set_xticks(ticks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    ax2.set_xticklabels(labels)\n",
    "    \n",
    "    # plot weights graph\n",
    "    ax2 = figure.add_subplot(8, 8, 3)\n",
    "    im2 = ax2.imshow(y_hat_train_arr_some[image_idx, :, :].T, cmap='Wistia')\n",
    "\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    cbar = figure.colorbar(im2, cax=cax, orientation='vertical', ticks=[0, 1])\n",
    "    cbar.ax.set_yticklabels(['0', '1'])\n",
    "\n",
    "    ax2.set(xlabel='Classes', ylabel='Iterations', title='Softmax Outputs')\n",
    "    ax2.set_xticks(ticks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    ax2.set_xticklabels(labels)\n",
    "    \n",
    "    # plot weights graph\n",
    "    ax2 = figure.add_subplot(8, 8, 4)\n",
    "    im2 = ax2.imshow(y_hat_train_arr_none[image_idx, :, :].T, cmap='Wistia')\n",
    "\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    cbar = figure.colorbar(im2, cax=cax, orientation='vertical', ticks=[0, 1])\n",
    "    cbar.ax.set_yticklabels(['0', '1'])\n",
    "\n",
    "    ax2.set(xlabel='Classes', ylabel='Iterations', title='Softmax Outputs')\n",
    "    ax2.set_xticks(ticks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    ax2.set_xticklabels(labels)\n",
    "\n",
    "    pdf.savefig(figure, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dba8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "## Compare all softmax matrices for one image ##\n",
    "################################################\n",
    "\n",
    "# define constants and params\n",
    "MAX_ITER = 25\n",
    "labels = [\"0\",  # index 0\n",
    "              \"1\",  # index 1\n",
    "              \"2\",  # index 2 \n",
    "              \"3\",  # index 3 \n",
    "              \"4\",  # index 4\n",
    "              \"5\",  # index 5\n",
    "              \"6\",  # index 6 \n",
    "              \"7\",  # index 7 \n",
    "              \"8\",  # index 8 \n",
    "              \"9\"]  # index 9\n",
    "\n",
    "# for every pretrained model, generate images for random, \n",
    "# semi random, and non random training processes\n",
    "for END_IDX in range(1, 11):\n",
    "    save_path = \"/scratch/gpfs/eysu/low_shot_weights/\" + str(END_IDX) \n",
    "    \n",
    "    x_train, x_valid, x_test, y_train, y_valid, y_test = loadData()\n",
    "    # create empty array to store softmax outputs\n",
    "    y_hat_train_arr_LR = np.zeros([y_train.shape[0], 10, MAX_ITER])\n",
    "    y_hat_train_arr_some = np.zeros([y_train.shape[0], 10, MAX_ITER])\n",
    "    y_hat_train_arr_none = np.zeros([y_train.shape[0], 10, MAX_ITER])\n",
    "    \n",
    "    # store the softmax vector from every iteration of training into y_hat_train_arr\n",
    "    for i in range(MAX_ITER):\n",
    "        \n",
    "        if i == 0:\n",
    "            y_hat_train_name = 'y_hat_train_seed'\n",
    "\n",
    "        else:\n",
    "            y_hat_train_name = 'y_hat_train_' + 'iter' + str(i)\n",
    "\n",
    "        # Load test set softmax outputs \n",
    "        yhtr_LR = np.load(save_path + \"/LR_adjusted/LR_adjusted\" + y_hat_train_name + '.npy')\n",
    "        yhtr_some = np.load(save_path + \"/shuffle_some/\" + y_hat_train_name + '.npy')\n",
    "        yhtr_none = np.load(save_path + \"/shuffle_none/\" + y_hat_train_name + '.npy')\n",
    "\n",
    "        y_hat_train_arr_LR[:, :, i] = yhtr_LR\n",
    "        y_hat_train_arr_some[:, :, i] = yhtr_some\n",
    "        y_hat_train_arr_none[:, :, i] = yhtr_none\n",
    "\n",
    "    save_image_path = \"Outputs/compare_all_methods/\" + str(END_IDX) + \".pdf\"\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(save_image_path)\n",
    "    for j in range(200):\n",
    "        visualize_all_softmax(np.random.randint(0, y_hat_train_arr.shape[0]))\n",
    "    pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7b441",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88684364",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "## Load in MNIST Digits ##\n",
    "##########################\n",
    "\n",
    "all_data = np.load(\"/scratch/gpfs/eysu/src_data/mnist.npz\")\n",
    "print(all_data.files)\n",
    "x_test = all_data['x_test']\n",
    "x_train = all_data['x_train']\n",
    "y_train = all_data['y_train']\n",
    "y_test = all_data['y_test']\n",
    "\n",
    "print(x_test.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# examine\n",
    "# print(x_train)\n",
    "# print(x_test)\n",
    "# print(y_train)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ffe2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## Partition and resize data ##\n",
    "###############################\n",
    "\n",
    "labels = [\"0\",  # index 0\n",
    "          \"1\",  # index 1\n",
    "          \"2\",  # index 2 \n",
    "          \"3\",  # index 3 \n",
    "          \"4\",  # index 4\n",
    "          \"5\",  # index 5\n",
    "          \"6\",  # index 6 \n",
    "          \"7\",  # index 7 \n",
    "          \"8\",  # index 8 \n",
    "          \"9\"]  # index 9\n",
    "\n",
    "# save train labels\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "\n",
    "# y_train_labels = y_train\n",
    "# y_test_labels = y_test\n",
    "\n",
    "# Further break training data into train / validation sets (# put 5000 into validation set and keep remaining 55,000 for train)\n",
    "(x_train, x_valid) = x_train[5000:], x_train[:5000] \n",
    "(y_train, y_valid) = y_train[5000:], np.array(y_train[:5000]).squeeze()\n",
    "\n",
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
    "\n",
    "# examine\n",
    "print(y_train.shape)\n",
    "print(np.unique(y_train, return_counts = True))\n",
    "print(y_train)\n",
    "print(x_train.shape)\n",
    "\n",
    "print(y_valid.shape)\n",
    "print(np.unique(y_valid, return_counts = True))\n",
    "print(y_valid)\n",
    "print(x_valid.shape)\n",
    "\n",
    "print(y_test.shape)\n",
    "print(np.unique(y_test, return_counts = True))\n",
    "print(y_test)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255be5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take subset of data\n",
    "END_IDX = 0\n",
    "\n",
    "idx_train = np.where(y_train < END_IDX)\n",
    "y_train_subset = y_train[idx_train]\n",
    "x_train_subset = x_train[idx_train]\n",
    "\n",
    "idx_valid = np.where(y_valid < END_IDX)\n",
    "y_valid_subset = y_valid[idx_valid]\n",
    "x_valid_subset = x_valid[idx_valid]\n",
    "\n",
    "idx_test = np.where(y_test < END_IDX)\n",
    "y_test_subset = y_test[idx_test]\n",
    "x_test_subset = x_test[idx_test]\n",
    "\n",
    "\n",
    "#examine\n",
    "print(y_train_subset.shape)\n",
    "print(np.unique(y_train_subset, return_counts = True))\n",
    "print(y_train_subset)\n",
    "print(x_train_subset.shape)\n",
    "\n",
    "print(y_valid_subset.shape)\n",
    "print(np.unique(y_valid_subset, return_counts = True))\n",
    "print(y_valid_subset)\n",
    "print(x_valid_subset.shape)\n",
    "\n",
    "print(y_test_subset.shape)\n",
    "print(np.unique(y_test_subset, return_counts = True))\n",
    "print(y_test_subset)\n",
    "print(x_test_subset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model: a small CNN model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Must define the input shape in the first layer of the neural network\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Take a look at the model summary\n",
    "# model.summary()\n",
    "\n",
    "# Number of epochs per training run\n",
    "EPOCHS = 10\n",
    "# Where to store output model weights and softmax predictions\n",
    "save_path = \"/scratch/gpfs/eysu/low_shot_weights/\"\n",
    "\n",
    "# fix dimensions\n",
    "y_train_subset = tf.keras.utils.to_categorical(y_train_subset, 10)\n",
    "y_valid_subset = tf.keras.utils.to_categorical(y_valid_subset, 10)\n",
    "y_test_subset = tf.keras.utils.to_categorical(y_test_subset, 10)\n",
    "\n",
    "print(np.unique(np.where(y_train_subset ==1)[1]))\n",
    "\n",
    "mpth = 'model.weights.best.pretrain.hdf5'\n",
    "y_hat_test_name = 'y_hat_test_pretrain'\n",
    "y_hat_train_name = 'y_hat_train_pretrain'\n",
    "\n",
    "\n",
    "# define optimization and energy parameters\n",
    "# set learning rate and exponential decay rate *** BORROWING FROM INTERLEAVING PAPER **** , \n",
    "opt = keras.optimizers.Adam(learning_rate=0.001, beta_1 =0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Save checkpoints\n",
    "checkpointer = ModelCheckpoint(filepath= save_path + mpth, verbose = 1, save_best_only=True) #True\n",
    "# Train the model\n",
    "model.fit(x_train_subset,\n",
    "         y_train_subset,\n",
    "         batch_size=64,\n",
    "         epochs=EPOCHS,\n",
    "         validation_data=(x_valid_subset, y_valid_subset),\n",
    "         callbacks=[checkpointer])\n",
    "\n",
    "# y_hat = model.predict(x_train_subset) #feed back serial reproduction targets\n",
    "y_hat_test = model.predict(x_test_subset)\n",
    "\n",
    "# Load the weights with the best validation accuracy\n",
    "model.load_weights(save_path + mpth)\n",
    "# Evaluate the model on test set\n",
    "score = model.evaluate(x_test_subset, y_test_subset, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1]) \n",
    "\n",
    "# Save results for each iteration in the serial reproduction chain\n",
    "np.save(save_path + y_hat_train_name + '.npy', y_train_subset)\n",
    "print(save_path + y_hat_train_name)\n",
    "\n",
    "np.save(save_path + y_hat_test_name + '.npy', y_hat_test)\n",
    "print(save_path + y_hat_test_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60edea4",
   "metadata": {},
   "source": [
    "# Begin the Iterated Learning Retraining Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "## Create a Shuffled y_train such that the training images ##\n",
    "## and labels no longer match up                           ##\n",
    "#############################################################\n",
    "y_train = all_data['y_train'][5000:]\n",
    "y_train_shuffle = np.copy(y_train)\n",
    "np.random.shuffle(y_train_shuffle)\n",
    "\n",
    "# Examine\n",
    "print(y_train)\n",
    "print(y_train_shuffle)\n",
    "print(y_train_shuffle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0bdc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "## Create a Shuffled y_train such that the training images ##\n",
    "## and labels no longer match up                           ##\n",
    "#############################################################\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = loadData()\n",
    "\n",
    "END_IDX = 9\n",
    "\n",
    "# indices of the images that the model was NOT pretrained on\n",
    "idx_train = np.where(y_train < END_IDX)[0]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_train_true = x_train[idx_train]\n",
    "y_train_true = y_train[idx_train]\n",
    "x_train_shuffle = np.delete(x_train, idx_train, axis=0)\n",
    "y_train_shuffle = np.delete(y_train, idx_train, axis=0)\n",
    "\n",
    "print(x_train_true.shape)\n",
    "print(y_train_true.shape)\n",
    "print(x_train_shuffle.shape)\n",
    "print(y_train_shuffle.shape)\n",
    "\n",
    "print(y_train_shuffle)\n",
    "np.random.shuffle(y_train_shuffle)\n",
    "print(y_train_shuffle)\n",
    "\n",
    "\n",
    "x_train = np.concatenate((x_train_true, x_train_shuffle), axis=0)\n",
    "y_train = np.concatenate((y_train_true, y_train_shuffle))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train)\n",
    "\n",
    "mix_order = np.random.permutation(len(y_train))\n",
    "x_train = x_train[mix_order]\n",
    "y_train = y_train[mix_order]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(idx_train)\n",
    "# print(y_train_shuffle[idx_train])\n",
    "# np.random.shuffle(y_train_shuffle[idx_train])\n",
    "# print(y_train_shuffle[idx_train])\n",
    "\n",
    "# y_train_shuffle = np.copy(y_train)\n",
    "# np.random.shuffle(y_train_shuffle[idx_train])\n",
    "\n",
    "# print(np.unique(y_train_shuffle - y_train))\n",
    "\n",
    "# idx_valid = np.where(y_valid < END_IDX)\n",
    "#     y_valid_subset = y_valid[idx_valid]\n",
    "#     x_valid_subset = x_valid[idx_valid]\n",
    "\n",
    "# idx_test = np.where(y_test < END_IDX)\n",
    "#     y_test_subset = y_test[idx_test]\n",
    "#     x_test_subset = x_test[idx_test]\n",
    "\n",
    "\n",
    "# y_train = all_data['y_train'][5000:]\n",
    "# y_train_shuffle = np.copy(y_train)\n",
    "# np.random.shuffle(y_train_shuffle)\n",
    "\n",
    "# print(y_train)\n",
    "# print(y_train_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a395e6e",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "- visualize the softmax matrices of 200 random images. One hot seed vectors should be random. Does the model perform better at categorizing each time? How will things converge?\n",
    "- quantify the final categories predicted. What is the distribution between the 10 classes?\n",
    "    - interesting thing to try might be line graphs showing the fraction of images in each class taken every 5 iterations. Hopefully will see changes in the lines over time?\n",
    "- What if we don't provide an input # classes? This is done thorugh the .to_categorical step. Look into if there is a way to let this be determined naturally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e3edc",
   "metadata": {},
   "source": [
    "# Initial Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "## Look at softmax output matrices for random images ##\n",
    "#######################################################\n",
    "# Number of learning iterations\n",
    "MAX_ITER = 25\n",
    "save_path = \"/scratch/gpfs/eysu/low_shot_weights/10/\"\n",
    "\n",
    "x_train, x_valid, x_test, y_train, y_valid, y_test = loadData()\n",
    "\n",
    "y_hat_train_arr = np.zeros([y_train.shape[0], 10, MAX_ITER])\n",
    "# y_hat_train_arr = np.zeros([y_train.shape[0], len(labels), MAX_ITER])\n",
    "for i in range(MAX_ITER):\n",
    "    if i == 0:\n",
    "        y_hat_train_name = 'LR_adjustedy_hat_train_seed'\n",
    "       \n",
    "    else:\n",
    "        y_hat_train_name = 'LR_adjustedy_hat_train_' + 'iter' + str(i)\n",
    "        \n",
    "    # Load test set softmax outputs \n",
    "    yhtr = np.load(save_path + y_hat_train_name + '.npy')\n",
    "\n",
    "    # The first time through, use binary weight vectors to save seed array\n",
    "    # Recall that these initial labels were randomized and do not correlate to \n",
    "    # the image's given class in the dataset\n",
    "    \n",
    "    if i == 0:\n",
    "        true_class_tr = np.nonzero(yhtr)[1]   \n",
    "    y_hat_train_arr[:, :, i] = yhtr\n",
    "\n",
    "print(y_hat_train_arr.shape)\n",
    "# (55000, 10, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"0\",  # index 0\n",
    "              \"1\",  # index 1\n",
    "              \"2\",  # index 2 \n",
    "              \"3\",  # index 3 \n",
    "              \"4\",  # index 4\n",
    "              \"5\",  # index 5\n",
    "              \"6\",  # index 6 \n",
    "              \"7\",  # index 7 \n",
    "              \"8\",  # index 8 \n",
    "              \"9\"]  # index 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715300a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "## Visualize N images ##\n",
    "########################\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.backends.backend_pdf\n",
    "\n",
    "def visualize_softmax(image_idx):\n",
    "\n",
    "    # visualize each image\n",
    "    figure = plt.figure(figsize=(40, 40))\n",
    "    # plot image\n",
    "    ax1 = figure.add_subplot(8, 8, 1, xticks=[], yticks=[])\n",
    "    im1 = ax1.imshow(x_train[image_idx])\n",
    "    ax1.set_title(\"Image\")\n",
    "\n",
    "    # plot weights graph\n",
    "    ax2 = figure.add_subplot(8, 8, 2)\n",
    "    im2 = ax2.imshow(y_hat_train_arr[image_idx, :, :].T, cmap='Wistia')\n",
    "\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    cbar = figure.colorbar(im2, cax=cax, orientation='vertical', ticks=[0, 1])\n",
    "    cbar.ax.set_yticklabels(['0', '1'])\n",
    "\n",
    "    ax2.set(xlabel='Classes', ylabel='Iterations', title='Softmax Outputs')\n",
    "    ax2.set_xticks(ticks=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    ax2.set_xticklabels(labels)\n",
    "\n",
    "    pdf.savefig(figure, bbox_inches='tight')\n",
    "    plt.show()\n",
    "        \n",
    "save_image_path = \"Outputs/\"\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(save_image_path + \"200_lo_shot_10_class_LR_adjusted.pdf\")\n",
    "for i in range(200):\n",
    "    visualize_softmax(np.random.randint(0, y_hat_train_arr.shape[0]))\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f122e0",
   "metadata": {},
   "source": [
    "# Examine output softmax predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(np.argmax(y_hat_train_arr[:, :, 24], axis=1)))\n",
    "\n",
    "# this shows us that the digit predicted with the highest probability is always 1 for every image in the training set\n",
    "# not spread equally across all classes -> this is just model bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OK THIS WORKS\n",
    "\n",
    "\n",
    "# Define the model: a small CNN model (could probably be done outside loop)\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Must define the input shape in the first layer of the neural network\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Take a look at the model summary\n",
    "# model.summary()\n",
    "\n",
    "# define optimization and energy parameters\n",
    "# default learning rate for adam is 0.001\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001, beta_1 = 0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Save checkpoints\n",
    "checkpointer = ModelCheckpoint(filepath= \"tester\", verbose = 1, save_best_only=True) #True\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "# y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
    "# y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "x_train = x_train_subset\n",
    "x_valid = x_valid_subset\n",
    "x_test = x_test_subset\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train_subset, 10)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid_subset, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test_subset, 10)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=64,\n",
    "         epochs=EPOCHS,\n",
    "         validation_data=(x_valid, y_valid),\n",
    "         callbacks=[checkpointer])\n",
    "\n",
    "# Load the weights with the best validation accuracy\n",
    "y_hat = model.predict(x_train) #feed back serial reproduction targets\n",
    "y_hat_test = model.predict(x_test)\n",
    "\n",
    "model.load_weights(\"tester\")\n",
    "# Evaluate the model on test set\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1])\n",
    "\n",
    "#     # Save results for each iteration in the serial reproduction chain\n",
    "#     np.save(save_path + y_hat_train_name + '.npy', y_train)\n",
    "#     print(save_path + y_hat_train_name)\n",
    "\n",
    "#     np.save(save_path + y_hat_test_name + '.npy', y_hat_test)\n",
    "#     print(save_path + y_hat_test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09534856",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_hat_train_arr[:, 1, 24]))\n",
    "\n",
    "# confirms that the predicted probability that the input image is of class 1 is always the same across all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae82119",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_hat_train_arr[:, 0, 24]))\n",
    "print(np.unique(y_hat_train_arr[:, 1, 24]))\n",
    "print(np.unique(y_hat_train_arr[:, 2, 24]))\n",
    "print(np.unique(y_hat_train_arr[:, 3, 24]))\n",
    "print(np.unique(y_hat_train_arr[:, 4, 24]))\n",
    "print(np.unique(y_hat_train_arr[:, 5, 24]))\n",
    "print(np.unique(y_hat_train_arr[:, 6, 24]))\n",
    "print(np.unique(y_hat_train_arr[:, 7, 24]))\n",
    "print(np.unique(y_hat_train_arr[:, 8, 24]))\n",
    "print(np.unique(y_hat_train_arr[:, 9, 24]))\n",
    "\n",
    "# confirms that the predicted probabilities for every class is the same regardless of the input images\n",
    "# the randomization of the labels prevents the model from ever converging in its classification predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a3b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu [~/.conda/envs/tf2-gpu/]",
   "language": "python",
   "name": "conda_tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
